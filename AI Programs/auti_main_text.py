# -*- coding: utf-8 -*-
"""Auti Main

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PCemuYnOH-F-hGAZ8ODBioRejaDFXsyv

**Questions**

A1: Does your child look at you when you call his/her name?

A2: How easy is it for you to get eye contact with your child?

A3: Does your child point to indicate that s/he wants something? (e.g. a toy that is out of reach)

A4: Does your child point to share interest with you? (e.g. pointing at an interesting sight)

A5: Does your child pretend? (e.g. care for dolls, talk on a toy phone)

A6: Does your child follow where you’re looking?

A7: If you or someone else in the family is visibly upset, does your child show signs of wanting to comfort them? (e.g. stroking hair, hugging them)

A8: Would you describe your child’s first words as something meaningful?

A9: Does your child use simple gestures? (e.g. wave goodbye)

A10: Does your child stare at nothing with no apparent purpose?
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler

from sklearn.metrics import classification_report, accuracy_score, f1_score, precision_score, recall_score, confusion_matrix
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
import tensorflow
from tensorflow import keras
from keras import Sequential
from keras.layers import Dense
from keras.callbacks import EarlyStopping
import os

from google.colab import drive
drive.mount('/content/gdrive')

path='/content/gdrive/MyDrive/HackSC/Toddler_Autism_dataset.csv'

df = pd.read_csv(path)

df1 = df.copy()

df.head()

df.info()

df = df.rename(columns={"Age_Mons":"Age Months",
                        "Family_mem_with_ASD":"Family Member with ASD",
                        "Class/ASD Traits ": "ASD Traits"})
df.columns

df.isnull().mean() * 100

df["Ethnicity"].value_counts()

df["Ethnicity"] = df["Ethnicity"].replace("mixed", "Others")
df["Ethnicity"] = df["Ethnicity"].replace("Native Indian", "South Asian")
df["Ethnicity"] = df["Ethnicity"].replace("asian", "Other Asians")
df["Ethnicity"] = df["Ethnicity"].replace("middle eastern", "Middle Eastern")
df["Ethnicity"] = df["Ethnicity"].replace("south asian", "South Asian")
df["Ethnicity"] = df["Ethnicity"].replace("black", "African")

df["Ethnicity"].value_counts()

df["Who completed the test"].unique()

df["Who completed the test"] = df["Who completed the test"].replace("family member", "Family Member")
df["Who completed the test"] = df["Who completed the test"].replace("Health care professional", "Health Care Professional")

df["Who completed the test"].value_counts()

cat_df = df[["Sex", "Ethnicity", "Jaundice", "Who completed the test", "Family Member with ASD", "ASD Traits"]]
num_df = df[["A1", "A2", "A3", "A4","A5","A6", "A7", "A8", "A9", "A10", "Qchat-10-Score", "Age Months"]]

for col in df.iloc[:, 13: ]:
    print(col,"\n", df[col].unique(), "\n")

df.describe()

"""#Data Preprocessing"""

df1.drop(columns=["Case_No"], inplace=True)

df1.drop(columns=["Who completed the test"], inplace=True)

df1.drop(columns=["Qchat-10-Score"], inplace=True)

mini = df1["Age_Mons"].min()
maxi = df1["Age_Mons"].max()
print(f"Minimum age {mini} and Maximum age {maxi}")

df1["Age"] = round(df1["Age_Mons"] / 12)
df1.drop(columns=["Age_Mons"], inplace=True)

df1.head()

order = ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10',  'Age',
         'Sex', 'Ethnicity', 'Jaundice', 'Family_mem_with_ASD',
         'Class/ASD Traits ']
df1 = df1[order]

df1.head()

df1.columns

df1["Class/ASD Traits "].unique()

df1["Class/ASD Traits "].value_counts()

le = LabelEncoder()
df1["Sex"] = le.fit_transform(df1["Sex"])
df1["Jaundice"] = le.fit_transform(df1["Jaundice"])
df1["Family_mem_with_ASD"] = le.fit_transform(df1["Family_mem_with_ASD"])
df1["Class/ASD Traits "] = le.fit_transform(df1["Class/ASD Traits "])
df1.head()

df1 = pd.get_dummies(df1, columns=["Ethnicity"], drop_first=True)
df1.head()

order = ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age',
       'Sex', 'Jaundice', 'Family_mem_with_ASD', 'Ethnicity_Latino',
        'Ethnicity_Native Indian','Ethnicity_Others', 'Ethnicity_Pacifica',
        'Ethnicity_White European','Ethnicity_asian', 'Ethnicity_black', 'Ethnicity_middle eastern',
       'Ethnicity_mixed', 'Ethnicity_south asian',
         'Class/ASD Traits ']

df1 = df1[order]
df1.head()

"""#Data Splitting"""

X = df1.drop(columns=["Class/ASD Traits "])
y = df1["Class/ASD Traits "]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

sc = MinMaxScaler()
X_train_scaled = sc.fit_transform(X_train)
X_test_scaled = sc.transform(X_test)

def train_model(model, X_train_scaled, y_train, X_test_scaled, y_test):

    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_test_scaled)
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)

    score_df = pd.DataFrame([[accuracy, precision, recall, f1]],
                            columns=["accuracy", "precision", "recall", "f1"])
    return score_df

model = LogisticRegression()

results = train_model(model, X_train_scaled, y_train, X_test_scaled, y_test)

results.index = ["Logistic Regression"]
results

model = Sequential()
model.add(Dense(32, activation="relu", input_dim=24))

#model.add(Dense(64, activation="relu"))

#model.add(Dense(32, activation="relu"))

model.add(Dense(16, activation="relu"))
model.add(Dense(1, activation="sigmoid"))

model.compile(optimizer="Adam", loss="binary_crossentropy", metrics=["accuracy"])

callback = EarlyStopping(
    monitor="val_loss",
    min_delta=0.001,
    patience=10,
    verbose=1,
    mode="auto",
    baseline=None,
    restore_best_weights=None
)

history = model.fit(X_train_scaled, y_train, batch_size=10, epochs=14, validation_split=0.2, callbacks=callback)

figure = plt.subplots(figsize=(10,5))

plt.subplot(1, 2, 1)
plt.plot(history.history["loss"], label="loss")
plt.plot(history.history["val_loss"], label="val_loss")
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history["accuracy"], label="accuracy")
plt.plot(history.history["val_accuracy"], label="val_accuracy")
plt.legend()

y_prob = model.predict(X_test_scaled)
y_pred = np.where(y_prob > 0.5, 1, 0)
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

nn_df = pd.DataFrame([[accuracy, precision, recall, f1]],
                     columns=['accuracy', 'precision', 'recall', 'f1'])
nn_df.index = ["Artificial Neural Network"]
results = results.append(nn_df)
results

results

model.save("ANN_predict.h5")

model.save("ANN_predict.keras")

x = [1,0,1,0,1,1,0,1,0,1,1,0,1,0,1,1,0,1,0,1,1,0,1,0]

new_model = tf.keras.models.load_model('/content/gdrive/MyDrive/HackSC/New_ANN_predict.h5')

x = np.array(x).reshape(24, 1)

y = model.predict(x.T)

if y > 0.5:
  y = 1
else:
  y=0
y = str(y)

type(y)